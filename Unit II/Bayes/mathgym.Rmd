---
title: "Math Gym Analysis"
output:
  html_document:
    df_print: paged
  pdf_document:
    df_print: kable
    number_sections: yes
geometry: left=0.5cm,right=0.5cm,top=0.5cm,bottom=1.5cm
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readr)
library(dplyr)
library(coda)
library(pROC)
library(splines)
source("src/rwmetrop.R")
source("src/gibbs-probit.R")
```

# Read the Data
```{r}
mathgym <- read_csv("data/mathgym.csv")
dat <- mathgym %>%
	filter(Quiz_Zero_Score != 'Missing') %>%
	mutate(Passed = factor(Grade %in% c('A', 'B', 'C'))) %>%
	mutate(Grade = ordered(Grade, c('F', 'D', 'C', 'B', 'A'))) %>%
	mutate(Course = factor(Course)) %>%
	mutate(Section = factor(Section)) %>%
	mutate(FYI = factor(FYI)) %>%
	mutate(Grade = factor(Grade)) %>%
	mutate(Quiz_Zero_Score = as.numeric(Quiz_Zero_Score))
head(dat)
summary(dat)
```

# Maximum Likelihood
Try probit regression using the MLE.
``` {r}
# Keep text from wrapping in the output
options(width=100)

glm.out <- glm(Passed ~ Course +
	# Section +
	FYI +
	bs(Days_Attended_Math_Gym, knots = c(5,15), degree = 1) +
	bs(Quiz_Zero_Score, knots = c(1,5,15,20), degree = 1),
	data = dat, family = binomial(link = "probit"), x = TRUE)
summary(glm.out)
pred.out <- predict(glm.out, type = "response")

X <- glm.out$x
Beta.hat <- coef(glm.out)

# Plot the fitted basis functions
idx1 <- grep("Days_Attended_Math_Gym", names(Beta.hat))
idx2 <- grep("Quiz_Zero_Score", names(Beta.hat))
plot(dat$Days_Attended_Math_Gym, X[,idx1] %*% Beta.hat[idx1], pch = 20)
plot(dat$Quiz_Zero_Score, X[,idx2] %*% Beta.hat[idx2], pch = 20)

roc.out <- roc(dat$Passed, pred.out)
auc(roc.out)
plot(roc.out)
```

# Metropolis-Hastings Sampler
Prepare Metropolis-Hastings: function to compute log of joint density and hyperparameters.
``` {r}
y <- as.integer(dat$Passed == TRUE)
n <- NROW(X)
d <- NCOL(X)

logpost <- function(parm, Data) {
	n <- nrow(Data$X)
	d <- ncol(Data$X)
	Beta <- parm[1:d]
	Beta.prior <- sum(dnorm(Beta, 0, sqrt(hyper$var.Beta), log = TRUE))
	prob <- pnorm(Data$X %*% Beta)
	ll <- dbinom(Data$y, 1, prob, log = TRUE)
	sum(ll) + Beta.prior
}

par.init <- rep(0, d)
Data <- list(y = y, X = X)
hyper <- list(var.Beta = 1000)
```

Run sampler.
``` {r, fig.height=7}
proposal <- list(var = solve(t(X) %*% X), scale = 1.0)
metrop.out <- rwmetrop(par.init, logpost, Data, proposal, R = 10000,
	burn = 1000, thin = 10)
print(metrop.out$accept)

Beta.mcmc <- mcmc(metrop.out$par)
plot(Beta.mcmc[,1:3])
# acf(Beta.mcmc[,1:3])
```

Compute summary of the posterior.
``` {r}
summary(Beta.mcmc)
```

Compute means from the posterior predictive distribution and use them to compute ROC curve.
``` {r}
R.keep <- NROW(Beta.mcmc)
y.hat <- numeric(n)

for (idx in 1:n) {
	x <- X[idx,]
	p.draws <- pnorm(X[idx,] %*% t(Beta.mcmc))
	y.draws <- rbinom(R.keep, size = 1, prob = p.draws)
	y.hat[idx] <- mean(y.draws)
}

head(cbind(dat, y.hat), n = 20)

roc.out <- roc(y, y.hat)
plot(roc.out)
print(roc.out)
```

# Gibbs Sampler
Run sampler.
``` {r, fig.height=7}
hyper <- list(V.prior = 1000 * diag(d))
gibbs.out <- gibbs.probit(y, X, R = 10000, burn = 1000, thin = 10, hyper = hyper)

Beta.mcmc <- mcmc(gibbs.out$Beta.hist)
plot(Beta.mcmc[,1:3])
# acf(Beta.mcmc[,1:3])
```

Compute summary of the posterior.
``` {r}
summary(Beta.mcmc)
```

Compute means from the posterior predictive distribution and use them to compute ROC curve.
``` {r}
R.keep <- NROW(Beta.mcmc)
y.hat <- numeric(n)

for (idx in 1:n) {
	x <- X[idx,]
	p.draws <- pnorm(X[idx,] %*% t(Beta.mcmc))
	y.draws <- rbinom(R.keep, size = 1, prob = p.draws)
	y.hat[idx] <- mean(y.draws)
}

head(cbind(dat, y.hat), n = 20)

roc.out <- roc(y, y.hat)
plot(roc.out)
print(roc.out)
```
